{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d38c97-b765-4e87-a866-e065742082d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18228270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch is up at http://es01:9200.\n"
     ]
    }
   ],
   "source": [
    "ES_HOST = os.environ.get(\"ES_HOST\", \"http://es01:9200\")\n",
    "WORDS_FILE = \"/app/words.txt\"\n",
    "\n",
    "def wait_for_elasticsearch():\n",
    "    \"\"\" Wait until Elasticsearch is accessible on ES_HOST. \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(ES_HOST, timeout=3)\n",
    "            if r.status_code == 200:\n",
    "                print(f\"Elasticsearch is up at {ES_HOST}.\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        print(f\"Waiting for Elasticsearch to be ready at {ES_HOST}...\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        \n",
    "wait_for_elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9ad41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the 'autocomplete' index with completion mapping...\n",
      "Index created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_autocomplete_index():\n",
    "    \"\"\" Create the 'autocomplete' index with a 'completion' field mapping. \"\"\"\n",
    "    print(\"Creating the 'autocomplete' index with completion mapping...\")\n",
    "\n",
    "    # Define the mapping for the 'suggest' field\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"suggest\": {\n",
    "                    \"type\": \"completion\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    url = f\"{ES_HOST}/autocomplete\"\n",
    "    # Use PUT to create or update index\n",
    "    response = requests.put(url, json=mapping)\n",
    "    if response.status_code in (200, 201):\n",
    "        print(\"Index created or updated successfully.\")\n",
    "    else:\n",
    "        # It's okay if index already exists; 400/404 can happen\n",
    "        print(f\"Index creation response ({response.status_code}): {response.text}\")\n",
    "\n",
    "create_autocomplete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26cad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the 'autocomplete' index with completion mapping...\n",
      "Index created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_autocomplete_index2():\n",
    "    \"\"\" Create the 'autocomplete' index with a 'completion' field mapping. \"\"\"\n",
    "    print(\"Creating the 'autocomplete' index with completion mapping...\")\n",
    "\n",
    "    # Define the mapping for the 'suggest' field\n",
    "    payload = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"folding_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\n",
    "                            \"lowercase\",\n",
    "                            \"asciifolding\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"suggest\": {\n",
    "                    \"type\": \"completion\",\n",
    "                    \"analyzer\": \"folding_analyzer\",\n",
    "                    \"preserve_separators\": True,\n",
    "                    \"preserve_position_increments\": True,\n",
    "                    \"max_input_length\": 50\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    url = f\"{ES_HOST}/autocomplete2\"\n",
    "    # Use PUT to create or update index\n",
    "    response = requests.put(url, json=payload)\n",
    "    if response.status_code in (200, 201):\n",
    "        print(\"Index created or updated successfully.\")\n",
    "    else:\n",
    "        # It's okay if index already exists; 400/404 can happen\n",
    "        print(f\"Index creation response ({response.status_code}): {response.text}\")\n",
    "\n",
    "create_autocomplete_index2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98842a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0:50000] indexed successfully.\n",
      "Batch [50000:100000] indexed successfully.\n",
      "Batch [100000:150000] indexed successfully.\n",
      "Batch [150000:200000] indexed successfully.\n",
      "Batch [200000:250000] indexed successfully.\n",
      "Batch [250000:300000] indexed successfully.\n",
      "Batch [300000:350000] indexed successfully.\n",
      "Batch [350000:400000] indexed successfully.\n",
      "Batch [400000:450000] indexed successfully.\n",
      "Batch [450000:466550] indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "def bulk_index_in_batches(index_name, words, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Splits the 'words' list into chunks of 'batch_size' and \n",
    "    sends each chunk to the Elasticsearch Bulk API.\n",
    "    \"\"\"\n",
    "    def generate_bulk_payload(batch):\n",
    "        lines = []\n",
    "        for word in batch:\n",
    "            # Action/metadata\n",
    "            lines.append(json.dumps({\"index\": {\"_index\": index_name}}))\n",
    "            # Document body\n",
    "            lines.append(json.dumps({\"suggest\": word}))\n",
    "        return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "    total_docs = len(words)\n",
    "    start = 0\n",
    "\n",
    "    while start < total_docs:\n",
    "        end = min(start + batch_size, total_docs)\n",
    "        batch = words[start:end]\n",
    "        payload = generate_bulk_payload(batch)\n",
    "\n",
    "        # Send bulk\n",
    "        bulk_url = f\"{ES_HOST}/_bulk?refresh=wait_for\"\n",
    "        headers = {\"Content-Type\": \"application/x-ndjson\"}\n",
    "        response = requests.post(bulk_url, data=payload, headers=headers)\n",
    "\n",
    "        if response.status_code not in (200, 201):\n",
    "            print(f\"Batch [{start}:{end}] failed with status {response.status_code}\")\n",
    "            print(response.text)\n",
    "        else:\n",
    "            resp_json = response.json()\n",
    "            if resp_json.get(\"errors\"):\n",
    "                print(f\"Batch [{start}:{end}] had partial failures:\")\n",
    "                for item in resp_json.get(\"items\", []):\n",
    "                    if \"error\" in item[\"index\"]:\n",
    "                        print(json.dumps(item[\"index\"][\"error\"], indent=2))\n",
    "            else:\n",
    "                print(f\"Batch [{start}:{end}] indexed successfully.\")\n",
    "\n",
    "        start = end\n",
    "\n",
    "def index_words_in_batches(index_name):\n",
    "    if not os.path.isfile(WORDS_FILE):\n",
    "        print(\"No words.txt file found; skipping.\")\n",
    "        return\n",
    "\n",
    "    with open(WORDS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # Here we choose 1000 as a default batch size; adjust to your needs\n",
    "    bulk_index_in_batches(index_name, words)\n",
    "\n",
    "# Then call:\n",
    "index_words_in_batches(\"autocomplete2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc622d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix: 'wo' -> Suggestions: ['WO', 'woa', 'woad', 'woad-leaved', 'woad-painted']\n",
      "Prefix: 'he' -> Suggestions: ['HE', 'he-all', 'he-balsam', 'he-broom', 'he-cabbage-tree']\n",
      "Prefix: 'wor' -> Suggestions: ['Worcester', 'Worcestershire', 'Word', 'worble', 'word-beat']\n",
      "Prefix: 'xyz' -> Suggestions: ['xyz']\n"
     ]
    }
   ],
   "source": [
    "def autocomplete(prefix):\n",
    "    \"\"\"\n",
    "    Send a suggest query to the 'autocomplete' index with the given prefix\n",
    "    and return a list of suggestion options.\n",
    "    \"\"\"\n",
    "    url = f\"{ES_HOST}/autocomplete/_search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Suggest query body\n",
    "    suggest_query = {\n",
    "        \"suggest\": {\n",
    "            \"word-suggest\": {\n",
    "                \"prefix\": prefix,\n",
    "                \"completion\": {\n",
    "                    \"field\": \"suggest\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=suggest_query)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching suggestions. Status: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # The suggest results are under data[\"suggest\"][\"word-suggest\"][0][\"options\"]\n",
    "    # We'll parse and return the suggested text.\n",
    "    suggestions = []\n",
    "    try:\n",
    "        suggest_items = data[\"suggest\"][\"word-suggest\"][0][\"options\"]\n",
    "        for item in suggest_items:\n",
    "            suggestions.append(item[\"text\"])\n",
    "    except (KeyError, IndexError):\n",
    "        print(\"Unexpected response format:\")\n",
    "        print(json.dumps(data, indent=2))\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "\n",
    "test_prefixes = [\"wo\", \"he\", \"wor\", \"xyz\"]\n",
    "\n",
    "for prefix in test_prefixes:\n",
    "    results = autocomplete(prefix)\n",
    "    print(f\"Prefix: '{prefix}' -> Suggestions: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a8498fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying prefix='exampel' with fuzziness=2...\n",
      "Prefix: 'exampel' -> Suggestions: ['examplar', 'example', \"example's\", 'exampled', 'exampleless']\n",
      "------------------------------------------------------------\n",
      "Querying prefix='exzmple' with fuzziness=2...\n",
      "Prefix: 'exzmple' -> Suggestions: ['examplar', 'example', \"example's\", 'exampled', 'exampleless']\n",
      "------------------------------------------------------------\n",
      "Querying prefix='workd' with fuzziness=2...\n",
      "Prefix: 'workd' -> Suggestions: ['Worcester', 'Worcestershire', 'Word', 'worble', 'word-beat']\n",
      "------------------------------------------------------------\n",
      "Querying prefix='hel' with fuzziness=2...\n",
      "Prefix: 'hel' -> Suggestions: ['H', 'H-bar', 'H-beam', 'H-blast', 'h.']\n",
      "------------------------------------------------------------\n",
      "Querying prefix='wonderful' with fuzziness=2...\n",
      "Prefix: 'wonderful' -> Suggestions: ['wonderful', 'wonderfuller', 'wonderfully', 'wonderfulness', 'wonderfulnesses']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fuzzy_autocomplete(index_name, prefix):    \n",
    "    max_fuzziness = 2  # 3 doesn't work, need another approach\n",
    "\n",
    "    print(f\"Querying prefix='{prefix}' with fuzziness={max_fuzziness}...\")\n",
    "\n",
    "    query = {\n",
    "        \"suggest\": {\n",
    "            \"autocomplete-suggest\": {\n",
    "                \"prefix\": prefix,\n",
    "                \"completion\": {\n",
    "                    \"field\": \"suggest\",\n",
    "                    \"skip_duplicates\": True,\n",
    "                    \"fuzzy\": {\n",
    "                        \"fuzziness\": max_fuzziness,\n",
    "                        \"transpositions\": True,\n",
    "                        \"min_length\": 2,    # minimum length before fuzzy\n",
    "                        \"prefix_length\": 1  # exact match for first character\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    url = f\"{ES_HOST}/{index_name}/_search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=query, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error searching. Status: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []\n",
    "    \n",
    "    data = response.json()\n",
    "    try:\n",
    "        # The path to suggestions -> data[\"suggest\"][\"autocomplete-suggest\"][0][\"options\"]\n",
    "        options = data[\"suggest\"][\"autocomplete-suggest\"][0][\"options\"]\n",
    "        return [opt[\"text\"] for opt in options]\n",
    "    except (KeyError, IndexError):\n",
    "        print(\"Unexpected search response format.\")\n",
    "        print(json.dumps(data, indent=2))\n",
    "        return []\n",
    "    \n",
    "\n",
    "test_prefixes = [\"exampel\", \"exzmple\", \"workd\", \"hel\", \"wonderful\"]\n",
    "for prefix in test_prefixes:\n",
    "    suggestions = fuzzy_autocomplete(\"autocomplete2\", prefix)\n",
    "    print(f\"Prefix: '{prefix}' -> Suggestions: {suggestions}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://stackoverflow.com/questions/53652482/elastic-search-with-fuzziness-more-than-2-characters-distance\n",
    "\n",
    "# I think fuzzy queries are inappropriate to your case. Fuzziness is a way to solve problem of little misspellings that human can make while typing his query. Human brain can easily skip substitution of some letter in the middle of word without loosing of overall meaning of phrase. The similar behavior we expect from search engine.\n",
    "\n",
    "# Try to use regular partial maching with ngrams analyzer:\n",
    "\n",
    "#     PUT my_index\n",
    "#     {\n",
    "#         \"settings\": {\n",
    "#             \"analysis\": {\n",
    "#                 \"filter\": {\n",
    "#                     \"trigrams_filter\": {\n",
    "#                         \"type\": \"ngram\",\n",
    "#                         \"min_gram\": 3,\n",
    "#                         \"max_gram\": 3\n",
    "#                     }\n",
    "#                 },\n",
    "#                 \"analyzer\": {\n",
    "#                     \"trigrams\": {\n",
    "#                         \"type\": \"custom\",\n",
    "#                         \"tokenizer\": \"standard\",\n",
    "#                         \"filter\": [\n",
    "#                             \"lowercase\",\n",
    "#                             \"trigrams_filter\"\n",
    "#                         ]\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         }, \n",
    "#         \"mappings\": {\n",
    "#             \"my_type\": {\n",
    "#                 \"properties\": {\n",
    "#                     \"my_field\": {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"analyzer\": \"trigrams\"\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
